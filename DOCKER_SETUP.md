# Setup Docker - Ollama + TraducaoTIME

## üìã Pr√©-requisitos

- Docker instalado ([Download](https://www.docker.com/products/docker-desktop))
- Docker Compose v2+
- Windows PowerShell ou WSL2 (recomendado para Windows)

## üöÄ Iniciando Ollama via Docker Compose

### 1. Iniciar apenas Ollama (mais r√°pido)

```bash
docker-compose up -d ollama
```

Isto ir√°:
- ‚úÖ Baixar a imagem do Ollama (~7GB)
- ‚úÖ Criar container `ollama-ia`
- ‚úÖ Expor na porta `11434`
- ‚úÖ Armazenar dados em volume persistente

### 2. Aguardar Ollama ficar pronto

```bash
# Verificar status
docker-compose ps

# Verificar logs
docker-compose logs -f ollama
```

O Ollama est√° pronto quando ver:
```
ollama-ia  | Listening on 127.0.0.1:11434
```

### 3. Puxar um modelo (primeira vez √© demorado)

```bash
# Entrar no container
docker-compose exec ollama ollama pull llama2

# Ou outras op√ß√µes:
docker-compose exec ollama ollama pull mistal
docker-compose exec ollama ollama pull neural-chat
```

**Primeira execu√ß√£o**: Pode levar 5-15 minutos (download de 4-10GB)

### 4. Testar Ollama

```powershell
# Windows PowerShell
Invoke-WebRequest -Uri "http://localhost:11434/api/tags" -Method Get

# Se funcionar, ver√° JSON com modelos dispon√≠veis
```

Ou via cURL:
```bash
curl http://localhost:11434/api/tags
```

---

## üåê Interface Web Opcional (Open WebUI)

Para usar uma interface web bonitinha para testar Ollama:

```bash
# Iniciar com WebUI
docker-compose --profile webui up -d

# Acessar em http://localhost:8080
```

---

## üîó Configura√ß√£o no seu Aplicativo

No arquivo `.env`:

```env
# Ativar Ollama
AI_PROVIDER=ollama
OLLAMA_API_URL=http://localhost:11434
OLLAMA_MODEL=llama2
# ou
OLLAMA_MODEL=mistral
OLLAMA_MODEL=neural-chat
OLLAMA_MODEL=orca-mini
```

---

## üìä Modelos Recomendados

| Modelo | Tamanho | Velocidade | Qualidade | Comando |
|--------|--------|-----------|----------|---------|
| **orca-mini** | 2GB | ‚ö°‚ö°‚ö° R√°pido | ‚≠ê‚≠ê | `ollama pull orca-mini` |
| **neural-chat** | 5GB | ‚ö°‚ö° M√©dio | ‚≠ê‚≠ê‚≠ê | `ollama pull neural-chat` |
| **mistral** | 4GB | ‚ö°‚ö° M√©dio | ‚≠ê‚≠ê‚≠ê‚≠ê | `ollama pull mistral` |
| **llama2** | 4GB | ‚ö° Lento | ‚≠ê‚≠ê‚≠ê‚≠ê | `ollama pull llama2` |

**Recomenda√ß√£o**: Comece com `neural-chat` ou `mistral`

---

## üõë Parando e Limpando

```bash
# Parar containers (mant√©m dados)
docker-compose down

# Parar e remover volumes (deleta tudo)
docker-compose down -v

# Parar apenas Ollama
docker-compose stop ollama
```

---

## üîç Troubleshooting

### Ollama n√£o consegue conectar

```bash
# Verificar se est√° rodando
docker-compose ps

# Verificar logs
docker-compose logs ollama

# Reiniciar
docker-compose restart ollama
```

### Porta 11434 j√° est√° em uso

Mude a porta no `docker-compose.yml`:
```yaml
ports:
  - "11435:11434"  # Use 11435 ao inv√©s de 11434
```

### Sem espa√ßo em disco (modelos s√£o grandes)

```bash
# Listar espa√ßo
docker system df

# Limpar cache Docker
docker system prune -a
```

### Container sai com erro

```bash
# Ver erro detalhado
docker-compose logs ollama --tail 50

# Tentar recriar
docker-compose down
docker-compose up --build ollama
```

---

## üí° Testes R√°pidos

### Teste via PowerShell

```powershell
$response = Invoke-WebRequest -Uri "http://localhost:11434/api/generate" `
  -Method Post `
  -Headers @{"Content-Type"="application/json"} `
  -Body '{"model":"llama2","prompt":"Ol√°, como voc√™ est√°?","stream":false}' `
  -UseBasicParsing

$response.Content | ConvertFrom-Json | Select -ExpandProperty response
```

### Teste via cURL (WSL/Git Bash)

```bash
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model":"llama2","prompt":"Ol√°, como voc√™ est√°?","stream":false}' \
  | jq '.response'
```

---

## üéØ Pr√≥ximos Passos

1. ‚úÖ Iniciar Ollama com `docker-compose up -d ollama`
2. ‚úÖ Puxar um modelo com `docker-compose exec ollama ollama pull mistral`
3. ‚úÖ Configurar `.env` com `AI_PROVIDER=ollama`
4. ‚úÖ Executar a aplica√ß√£o
5. ‚úÖ Usar o menu "IA" para fazer perguntas sobre a conversa

---

## üìö Recursos

- [Ollama Docs](https://github.com/ollama/ollama)
- [Open WebUI](https://github.com/open-webui/open-webui)
- [Modelos Dispon√≠veis](https://ollama.ai/library)
